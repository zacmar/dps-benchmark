# Diffusion Posterior Sampling Benchmark

### Prerequisites
The code relies on CUDA implementations of some sampling algorithms and consequently requires CUDA-capable GPUs.

### Setup
The pipeline assumes that the environment variable `EXPERIMENTS_ROOT` points to a path that points to a storage device that has sufficient space.
A full run of the pipeline requires about 171 gigabytes of storage.

### Pipeline
The pipeline is compartmentalized into stages and each stage typically has an associated python file.
These files are equipped with an argument parser that, where applicable, allows the specification of
1. the forward operator (`identity`, `convolution`, `sample`)
2. the jump distribution and its parameters (`gauss`, `laplace`, `student`, `bernoulli-laplace`). The `bernoulli-laplace` distribution has 2 parameters, all others have one parameter.
3. the DPS algorithm (`cdps`, `diffpir`, `dpnp`) and the denoiser (`learned`, `gibbs`).
This enables a paralellization of the various components on multiple compute units.

The pipeline goes as follows.
1. Data generation: First, training, validation, and test signals with a specified jump distribution are synthesizes. Already at this stage, for the test signals, the measurement process is simulated and gold-standard posterior samples are obtained from the Gibbs methods. This stage is fairly fast and not parallelized. The data can be generated by running
```
python generate-datasets.py
```
The standard jump distributions are `Gauss(0, 0.25)`, `Laplace(1)`, `Student(nu)` for `nu=1,2,3`, and `Bernoulli-Laplace(0.1, 1)`

2. Training of the denoisers: The training signals are used to train standard noise-conditional score networks. An example launch looks like
```
python train.py bernoulli-laplace 0.1 1
```
The output of this stage are learned denoisers for the specified jump distributions.

3. Parameter search for the model-based methods and the DPS algorithms: The validation signals are used to find various parameters of the model-based methods and the DPS algorithms. The parameter grids are specified in the python file and produce reasonable results for the "standard" jump distributions specified in `generate-datasets.py`. They might need to be adapted for more exotic distributions. The grid search can be parallelized over the forward operator as well as the jump distribution, but runs for all methods (model-based and DPS) sequentially. For now, the parameters are found wrt. the learned denoiser. An example launch looks like
```
python grid-search.py convolution student 1
```
Since the model-based methods are not computationally heavy, we immediately infer the estimatiors for the test data also at this stage with the found optimal parameters.
Thus, the output of this stage are the validation-MSE values of the model-based methods and the DPS algorithms at the corresponding grid points and the estimations of the signals from the test data of the model-based methods.

4. Posterior sampling using the DPS algorithms: After the identification of the optimal parameters using the validation set, the DPS algorithms can be used to sample the posteriors that arise from the test data. The framework automatically loads the parameter grids and the associated validation-MSE and extracts the optimal parameters. This section of the pipeline is the most computationally demanding and launches are parametrized by the forward operator, the jump distribution, the DPS algorithm, and the used denoiser. An example launch looks like
```
python posterior-sampling.py identity laplace 1 diffpir gibbs
```
The output of this stage are the posterior samples obtained by the DPS algorithms.

5. Evaluation and visualization: At this point, we have gold-standard posterior samples, DPS samples, and model-based MMSE estimates on disc. The evaluation is done with the scripts in the `postprocessing` folder. For instance, to generate the table that contains the main results, the (negative log of the) MMSE optimality gap on the test data, we run
```
python -m postprocessing.mmse-gap-latex-table
```
The data for the plots of the main results in the paper can be generated by
```
python -m postprocessing.posterior-figure-data
```